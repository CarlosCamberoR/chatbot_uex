# ğŸ“ Chatbot Universidad de Extremadura

Un chatbot inteligente avanzado que utiliza IA y procesamiento de lenguaje natural para responder preguntas sobre la Universidad de Extremadura (UEx). El sistema extrae informaciÃ³n actualizada del sitio web oficial y documentos PDF para proporcionar respuestas precisas y contextuales.

## âœ¨ CaracterÃ­sticas Principales

- ğŸ•·ï¸ **Web Scraping Avanzado**: Extrae contenido automÃ¡ticamente del sitio web de la UEx incluyendo documentos PDF
- ğŸ§  **IA Conversacional**: Sistema de respuestas contextuales inteligente sin dependencia de modelos generativos problemÃ¡ticos
- ğŸ“š **Base de Conocimiento Vectorial**: Sistema de bÃºsqueda semÃ¡ntica con ChromaDB y embeddings multilingÃ¼es
- ğŸ“„ **Procesamiento de PDFs**: Capacidad para extraer y procesar informaciÃ³n de documentos PDF oficiales
- ğŸ¯ **Filtrado de Dominio**: Solo responde preguntas relacionadas con la UEx con detecciÃ³n automÃ¡tica de temas
- ğŸŒ **Interfaz Web Moderna**: AplicaciÃ³n Streamlit con diseÃ±o atractivo y funcional
- ğŸ“Š **EstadÃ­sticas en Tiempo Real**: Monitoreo del estado del sistema y mÃ©tricas de contenido

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### Prerrequisitos

- Python 3.8+ (recomendado 3.10)
- Conda o pip para gestiÃ³n de paquetes
- Token de Hugging Face (opcional, para funcionalidades avanzadas)

### 1. Clonar e instalar dependencias

```bash
git clone <repo-url>
cd chatbot_uex

# Crear entorno conda (recomendado)
conda create -n chatbot_uex python=3.10
conda activate chatbot_uex

# Instalar dependencias
pip install -r requirements.txt
```

### 2. Configurar variables de entorno

Crear archivo `.env` en la raÃ­z del proyecto:

```env
HUGGINGFACE_TOKEN=tu_token_aqui  # Opcional
```

### 3. Ejecutar el sistema completo

#### Paso 1: Extraer datos (Web Scraping)
```bash
python web_scraper.py
```
Este proceso:
- Extrae contenido de ~150 pÃ¡ginas del sitio web de la UEx
- Procesa documentos PDF automÃ¡ticamente
- Genera el archivo `unex_content.json` con toda la informaciÃ³n

#### Paso 2: Crear base de conocimiento
```bash
python knowledge_base.py
```
Este proceso:
- Procesa el contenido extraÃ­do en chunks semÃ¡nticos
- Crea embeddings multilingÃ¼es optimizados
- Genera la base de datos vectorial en `chroma_db/`

#### Paso 3: Ejecutar la interfaz web
```bash
streamlit run app.py
```
La aplicaciÃ³n estarÃ¡ disponible en: `http://localhost:8501`

## ğŸ—ï¸ Arquitectura del Sistema

### Componentes Principales

1. **`web_scraper.py`** - Sistema de extracciÃ³n de contenido
   - Scraping inteligente de pÃ¡ginas web
   - Procesamiento automÃ¡tico de PDFs con PyPDF2
   - Filtrado y limpieza de contenido
   - EstadÃ­sticas detalladas del proceso

2. **`knowledge_base.py`** - Base de conocimiento vectorial
   - Chunking semÃ¡ntico de documentos
   - Embeddings con `paraphrase-multilingual-MiniLM-L12-v2`
   - Almacenamiento en ChromaDB
   - Sistema de bÃºsqueda por similitud

3. **`chatbot.py`** - Motor conversacional
   - Sistema de respuestas contextuales
   - ClasificaciÃ³n automÃ¡tica de preguntas
   - Respuestas estructuradas en Markdown
   - DetecciÃ³n de preguntas fuera del dominio

4. **`app.py`** - Interfaz de usuario
   - AplicaciÃ³n Streamlit moderna
   - Chat interactivo en tiempo real
   - Botones de preguntas frecuentes
   - ExportaciÃ³n de conversaciones

5. **`config.py`** - ConfiguraciÃ³n centralizada
   - ParÃ¡metros del sistema
   - ConfiguraciÃ³n de modelos
   - Rutas y constantes

### Estructura del Proyecto

```
chatbot_uex/
â”œâ”€â”€ .env                    # Variables de entorno
â”œâ”€â”€ README.md              # DocumentaciÃ³n
â”œâ”€â”€ requirements.txt       # Dependencias del proyecto
â”œâ”€â”€ config.py             # ConfiguraciÃ³n centralizada
â”œâ”€â”€ web_scraper.py        # Extractor de contenido con soporte PDF
â”œâ”€â”€ knowledge_base.py     # Base de datos vectorial
â”œâ”€â”€ chatbot.py           # Motor conversacional inteligente
â”œâ”€â”€ app.py               # Interfaz web Streamlit
â”œâ”€â”€ unex_content.json    # Datos extraÃ­dos (generado automÃ¡ticamente)
â””â”€â”€ chroma_db/           # Base de datos vectorial (generada automÃ¡ticamente)
    â”œâ”€â”€ chroma.sqlite3
    â””â”€â”€ [archivos de Ã­ndices vectoriales]
```

## ğŸ’¡ Ejemplos de Uso

### Preguntas que puede responder:

- **Estudios**: "Â¿QuÃ© grados puedo estudiar en la UEx?"
- **Campus**: "Â¿DÃ³nde estÃ¡n ubicados los campus universitarios?"
- **MatrÃ­cula**: "Â¿CÃ³mo me matriculo en un grado?"
- **PAU**: "Â¿CÃ³mo funciona la prueba de acceso universitario?"
- **Becas**: "Â¿QuÃ© becas estÃ¡n disponibles para estudiantes?"
- **Servicios**: "Â¿QuÃ© servicios ofrece la biblioteca universitaria?"
- **InvestigaciÃ³n**: "Â¿QuÃ© grupos de investigaciÃ³n hay en la UEx?"
- **Noticias**: "Â¿CuÃ¡les son las Ãºltimas noticias de la universidad?"

## ğŸ”§ CaracterÃ­sticas TÃ©cnicas

### Web Scraping Inteligente
- **Cobertura**: ~150 pÃ¡ginas del ecosistema unex.es
- **Formatos**: HTML y PDF
- **Filtrado**: Contenido relevante y limpieza automÃ¡tica
- **Respeto**: Pausas entre requests y headers apropiados

### Base de Conocimiento Avanzada
- **Motor**: ChromaDB para bÃºsqueda vectorial
- **Embeddings**: Modelo multilingÃ¼e optimizado para espaÃ±ol
- **Chunks**: SegmentaciÃ³n semÃ¡ntica inteligente
- **BÃºsqueda**: Top-k por similitud coseno

### Sistema Conversacional
- **Enfoque**: Respuestas contextuales sin modelos generativos problemÃ¡ticos
- **ClasificaciÃ³n**: DetecciÃ³n automÃ¡tica de tipos de preguntas
- **Formato**: Respuestas estructuradas en Markdown
- **Dominio**: Filtrado inteligente de preguntas relevantes

### Interfaz Moderna
- **Framework**: Streamlit con diseÃ±o personalizado
- **Funciones**: Chat en tiempo real, preguntas frecuentes, exportaciÃ³n
- **Responsive**: Adaptable a diferentes dispositivos
- **EstadÃ­sticas**: MÃ©tricas del sistema en tiempo real

## ğŸ› ï¸ PersonalizaciÃ³n y ExtensiÃ³n

### Modificar el alcance del scraping
```python
# En web_scraper.py
max_pages = 200  # Aumentar pÃ¡ginas a procesar
```

### Ajustar parÃ¡metros de bÃºsqueda
```python
# En knowledge_base.py
chunk_size = 800  # TamaÃ±o de chunks
n_results = 5     # NÃºmero de resultados relevantes
```

### Personalizar tipos de preguntas
```python
# En chatbot.py - agregar nuevas categorÃ­as
question_types = {
    "tu_categoria": ["palabra1", "palabra2", "palabra3"]
}
```

## ğŸ“Š Monitoreo y EstadÃ­sticas

El sistema proporciona estadÃ­sticas detalladas:

- **Scraping**: PÃ¡ginas procesadas, PDFs extraÃ­dos, palabras totales
- **Base de conocimiento**: NÃºmero de chunks, embeddings generados
- **Chat**: Consultas procesadas, tipos de preguntas detectadas
- **Sistema**: Estado de componentes, tiempo de respuesta

## ğŸš¨ SoluciÃ³n de Problemas

### Problema: Base de conocimiento vacÃ­a
```bash
# SoluciÃ³n: Regenerar datos
rm -rf chroma_db/ unex_content.json
python web_scraper.py
python knowledge_base.py
```

### Problema: Streamlit no inicia
```bash
# Verificar configuraciÃ³n
pip install --upgrade streamlit
streamlit config show
```

### Problema: Error de memoria
```bash
# Reducir parÃ¡metros en config.py
chunk_size = 500  # Reducir tamaÃ±o de chunks
max_pages = 50    # Reducir pÃ¡ginas a procesar
```

## ğŸ“¦ Dependencias Principales

- **`transformers`**: Modelos de IA y tokenizaciÃ³n
- **`sentence-transformers`**: Embeddings semÃ¡nticos multilingÃ¼es
- **`chromadb`**: Base de datos vectorial
- **`streamlit`**: Framework de interfaz web
- **`beautifulsoup4`**: Parsing de HTML
- **`PyPDF2`**: Procesamiento de documentos PDF
- **`requests`**: Cliente HTTP para scraping

## ğŸ”„ Actualizaciones y Mantenimiento

### Actualizar contenido
```bash
# Ejecutar scraping periÃ³dicamente
python web_scraper.py
python knowledge_base.py
```

### Backup de datos
```bash
# Respaldar base de conocimiento
cp -r chroma_db/ chroma_db_backup/
cp unex_content.json unex_content_backup.json
```

## ğŸ¯ Roadmap Futuro

- [ ] IntegraciÃ³n con APIs oficiales de la UEx
- [ ] Soporte para mÃ¡s formatos de documentos (Word, Excel)
- [ ] Sistema de feedback de usuarios
- [ ] MÃ©tricas avanzadas de satisfacciÃ³n
- [ ] IntegraciÃ³n con sistemas de chat empresariales
- [ ] Modo offline completo

## ğŸ“„ Licencia

Este proyecto estÃ¡ desarrollado para fines educativos y de investigaciÃ³n relacionados con la Universidad de Extremadura.

---

**Â¡Disfruta explorando la Universidad de Extremadura con IA! ğŸ“âœ¨**

Para soporte o preguntas sobre el desarrollo, consulta la documentaciÃ³n tÃ©cnica en los archivos de cÃ³digo fuente.
